{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from minsearch import Index\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "documents = []\n",
    "\n",
    "with open('documents.json', 'r') as file:\n",
    "\n",
    "    docs = json.load(file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "\n",
    "    for document in doc['documents']:\n",
    "\n",
    "        document['course'] = doc['course'] \n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x733840123e20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfields = [\"text\", \"section\", \"question\"]\n",
    "\n",
    "\n",
    "indobject = Index(text_fields = textfields, keyword_fields = ['course'])\n",
    "indobject.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def search(query, boost_dict = {\"question\": 3}, filter_dict =  {\"course\":\"mlops-zoomcamp\"} ):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    context = indobject.search(query = query, boost_dict = boost_dict, filter_dict = filter_dict\n",
    "                    )\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, related_docs):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an assistant for teaching online courses answer the question based on the context provided.\n",
    "    Only the information available in the provided context should be used strictly adhere to this\n",
    "    \n",
    "    question:{query}\n",
    "    \n",
    "    context:{context}\n",
    "    \n",
    "    \n",
    "    \"\"\".strip()\n",
    "    \n",
    "    \n",
    "    context = \"\"\n",
    "    for doc in related_docs:\n",
    "        \n",
    "        context += f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']} \\n\\n\"\n",
    "        \n",
    "    \n",
    "    prompt = prompt.format(query = query, context = context).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, model = \"phi3\"):\n",
    "   \n",
    "    client = openai.OpenAI(\n",
    "        base_url = \"http://localhost:11434/v1/\",\n",
    "        \n",
    "        api_key = \"ollama\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [{\"role\":\"system\", \"content\":\"you are a faq assistant\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, boost_dict = {\"question\": 3}, course_filter =  {\"course\": \"data-engineering-zoomcamp\"}, model = \"phi3\"):\n",
    "    \n",
    "    context = search(query = query, boost_dict = boost_dict, filter_dict = course_filter)\n",
    "    \n",
    "    prompt = build_prompt(query, context)\n",
    "    \n",
    "    answer = chat(prompt, model = model)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-196', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To set Python as a startup on Windows, you can add it in `startup` folder within the users\\' path directories. Here is how to properly configure this setting for multiple common cases when starting up both local applications and Spark:\\n\\nFor **Local Applications** using Anaconda (Python-based):\\n```batchfile\\nC:\\\\Users\\\\[username]\\\\anaconda3_20*b*64*\\\\Scripts> echo %PYTHONHOME%\\n<empty> # Often you might get \\'NULL\\', not set.\\n\\nREM Set PYTHONPATH to work with libraries/packages under your local Anaconda environment directory and add it as a startup command after setting the path for convenience:\\nsetx SZ_USERPY THISUSERDIR \"%UserProfile%\\\\AppData\\\\Local\\\\<username>\\\\Anaconda3_20*b*64\\\\\" >NUL /M 1>NUL || set \"SZ_USERPY=%AppData%\\\\Local\\\\<userprofilename>>\";\\nsetx SZ_STARTUP %APPDATA%/Startup/%SystemRoot%/system32;C:\\\\Users\\\\\\\\[username]\\\\.anaconda3_20*b*64*\\\\;SZ_USERPY >>NUL 1>NUL || set \"SZ_STARTUP=%UserProfile%.local\\\\startup\"\\ncd /d %SZ_STARTUP%\\nif not \"%CD%\"==\"%~dp0\" cd /d %~dp0 && title StartUp & your python application.py > nul 2>&1 && goto EndStartUp || Echo Application may take a while to start in this folder, and if it fails on the first day after setting PYTHONHOME then set PATH=%PATH% >>SZ_STARTUP\\n:EndStartUp\\n```\\nRemember that you should never modify `PYTHONPATH` directly as Python will prompt for permission changes. You must define your own personalized directory path like above, which I call SZ_(variables), to avoid problems in the future when setting up environments on multiple computers or reorganizing project directories within a single computer without losing installed packages and environment settings from existing ones using Anaconda.\\nPlease don\\'t use `setx` for creating startup paths – you can always specify them directly as shown above under SZ_STARTUP, but they will be wiped out when formatting the hard drive or performing updates with Microsoft operating systems like Windows 10 & Linux Mint to name a few.\\nHere is an example of how one would start their local Pyspark Jupyter session every time after logging in:\\n```batchfile\\nREM For Local Spark Session using Anaconda3 + PYSPARK KERNEL or PySpark Standalone Mode (local):\\npython %PySparkHome%\\\\lib\\\\pyspark.zip\\\\pyspark\\\\shell\\\\launcher.py --conf \"spark.driver.memory 4G\" --properties-file spark_defaults.cfg python pyspark_script_locally.py && title PysparrKernal Session\\nREM Or directly start Jupyter Notebook from a standalone Spark Shell Command prompt using PySpark on Linux (or Windows/Mac when installed with Anaconda):  Start Python3 and type `python -m ipykernel install` afterwards to create the kernels in your system paths, as well.\\n```\\nFor **PySpark Standalone Mode**: Use above instructions from my answer by @tschija under *Project* section of Module 5 homework on Google CodeJam website but adapt it for Windows where necessary and replace `.bashrc` file content with `export SPARK_HOME=\"/spark-3.4.2\"`\\n```batchfile\\n...and start Jupyter Notebook: open a CMD terminal in administrator mode, cd %SPARK_HOME% (e.g., ./bin/linux/) , Enter and execute Startup command by itself if it’s not running already to launch Spark UI via `bin\\\\spark-class org.apache.spark.deploy.daemon.SparkAppFileServer --config /your path here`;\\nif you need, copy Jupyter Notebook extension code file from https://github.com/DataTalksClub/data-engineering-zoomcamp and upload with `git push origin master`. ', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1723306283, model='phi3', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=940, prompt_tokens=1026, total_tokens=1966))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To set Python as a startup on Windows, you can add it in `startup` folder within the users\\' path directories. Here is how to properly configure this setting for multiple common cases when starting up both local applications and Spark:\\n\\nFor **Local Applications** using Anaconda (Python-based):\\n```batchfile\\nC:\\\\Users\\\\[username]\\\\anaconda3_20*b*64*\\\\Scripts> echo %PYTHONHOME%\\n<empty> # Often you might get \\'NULL\\', not set.\\n\\nREM Set PYTHONPATH to work with libraries/packages under your local Anaconda environment directory and add it as a startup command after setting the path for convenience:\\nsetx SZ_USERPY THISUSERDIR \"%UserProfile%\\\\AppData\\\\Local\\\\<username>\\\\Anaconda3_20*b*64\\\\\" >NUL /M 1>NUL || set \"SZ_USERPY=%AppData%\\\\Local\\\\<userprofilename>>\";\\nsetx SZ_STARTUP %APPDATA%/Startup/%SystemRoot%/system32;C:\\\\Users\\\\\\\\[username]\\\\.anaconda3_20*b*64*\\\\;SZ_USERPY >>NUL 1>NUL || set \"SZ_STARTUP=%UserProfile%.local\\\\startup\"\\ncd /d %SZ_STARTUP%\\nif not \"%CD%\"==\"%~dp0\" cd /d %~dp0 && title StartUp & your python application.py > nul 2>&1 && goto EndStartUp || Echo Application may take a while to start in this folder, and if it fails on the first day after setting PYTHONHOME then set PATH=%PATH% >>SZ_STARTUP\\n:EndStartUp\\n```\\nRemember that you should never modify `PYTHONPATH` directly as Python will prompt for permission changes. You must define your own personalized directory path like above, which I call SZ_(variables), to avoid problems in the future when setting up environments on multiple computers or reorganizing project directories within a single computer without losing installed packages and environment settings from existing ones using Anaconda.\\nPlease don\\'t use `setx` for creating startup paths – you can always specify them directly as shown above under SZ_STARTUP, but they will be wiped out when formatting the hard drive or performing updates with Microsoft operating systems like Windows 10 & Linux Mint to name a few.\\nHere is an example of how one would start their local Pyspark Jupyter session every time after logging in:\\n```batchfile\\nREM For Local Spark Session using Anaconda3 + PYSPARK KERNEL or PySpark Standalone Mode (local):\\npython %PySparkHome%\\\\lib\\\\pyspark.zip\\\\pyspark\\\\shell\\\\launcher.py --conf \"spark.driver.memory 4G\" --properties-file spark_defaults.cfg python pyspark_script_locally.py && title PysparrKernal Session\\nREM Or directly start Jupyter Notebook from a standalone Spark Shell Command prompt using PySpark on Linux (or Windows/Mac when installed with Anaconda):  Start Python3 and type `python -m ipykernel install` afterwards to create the kernels in your system paths, as well.\\n```\\nFor **PySpark Standalone Mode**: Use above instructions from my answer by @tschija under *Project* section of Module 5 homework on Google CodeJam website but adapt it for Windows where necessary and replace `.bashrc` file content with `export SPARK_HOME=\"/spark-3.4.2\"`\\n```batchfile\\n...and start Jupyter Notebook: open a CMD terminal in administrator mode, cd %SPARK_HOME% (e.g., ./bin/linux/) , Enter and execute Startup command by itself if it’s not running already to launch Spark UI via `bin\\\\spark-class org.apache.spark.deploy.daemon.SparkAppFileServer --config /your path here`;\\nif you need, copy Jupyter Notebook extension code file from https://github.com/DataTalksClub/data-engineering-zoomcamp and upload with `git push origin master`. '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query = \"How to run spark engine ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set Python as a startup on Windows, you can add it in `startup` folder within the users' path directories. Here is how to properly configure this setting for multiple common cases when starting up both local applications and Spark:\n",
      "\n",
      "For **Local Applications** using Anaconda (Python-based):\n",
      "```batchfile\n",
      "C:\\Users\\[username]\\anaconda3_20*b*64*\\Scripts> echo %PYTHONHOME%\n",
      "<empty> # Often you might get 'NULL', not set.\n",
      "\n",
      "REM Set PYTHONPATH to work with libraries/packages under your local Anaconda environment directory and add it as a startup command after setting the path for convenience:\n",
      "setx SZ_USERPY THISUSERDIR \"%UserProfile%\\AppData\\Local\\<username>\\Anaconda3_20*b*64\\\" >NUL /M 1>NUL || set \"SZ_USERPY=%AppData%\\Local\\<userprofilename>>\";\n",
      "setx SZ_STARTUP %APPDATA%/Startup/%SystemRoot%/system32;C:\\Users\\\\[username]\\.anaconda3_20*b*64*\\;SZ_USERPY >>NUL 1>NUL || set \"SZ_STARTUP=%UserProfile%.local\\startup\"\n",
      "cd /d %SZ_STARTUP%\n",
      "if not \"%CD%\"==\"%~dp0\" cd /d %~dp0 && title StartUp & your python application.py > nul 2>&1 && goto EndStartUp || Echo Application may take a while to start in this folder, and if it fails on the first day after setting PYTHONHOME then set PATH=%PATH% >>SZ_STARTUP\n",
      ":EndStartUp\n",
      "```\n",
      "Remember that you should never modify `PYTHONPATH` directly as Python will prompt for permission changes. You must define your own personalized directory path like above, which I call SZ_(variables), to avoid problems in the future when setting up environments on multiple computers or reorganizing project directories within a single computer without losing installed packages and environment settings from existing ones using Anaconda.\n",
      "Please don't use `setx` for creating startup paths – you can always specify them directly as shown above under SZ_STARTUP, but they will be wiped out when formatting the hard drive or performing updates with Microsoft operating systems like Windows 10 & Linux Mint to name a few.\n",
      "Here is an example of how one would start their local Pyspark Jupyter session every time after logging in:\n",
      "```batchfile\n",
      "REM For Local Spark Session using Anaconda3 + PYSPARK KERNEL or PySpark Standalone Mode (local):\n",
      "python %PySparkHome%\\lib\\pyspark.zip\\pyspark\\shell\\launcher.py --conf \"spark.driver.memory 4G\" --properties-file spark_defaults.cfg python pyspark_script_locally.py && title PysparrKernal Session\n",
      "REM Or directly start Jupyter Notebook from a standalone Spark Shell Command prompt using PySpark on Linux (or Windows/Mac when installed with Anaconda):  Start Python3 and type `python -m ipykernel install` afterwards to create the kernels in your system paths, as well.\n",
      "```\n",
      "For **PySpark Standalone Mode**: Use above instructions from my answer by @tschija under *Project* section of Module 5 homework on Google CodeJam website but adapt it for Windows where necessary and replace `.bashrc` file content with `export SPARK_HOME=\"/spark-3.4.2\"`\n",
      "```batchfile\n",
      "...and start Jupyter Notebook: open a CMD terminal in administrator mode, cd %SPARK_HOME% (e.g., ./bin/linux/) , Enter and execute Startup command by itself if it’s not running already to launch Spark UI via `bin\\spark-class org.apache.spark.deploy.daemon.SparkAppFileServer --config /your path here`;\n",
      "if you need, copy Jupyter Notebook extension code file from https://github.com/DataTalksClub/data-engineering-zoomcamp and upload with `git push origin master`. \n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to run spark engine ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = search(query = query, filter_dict = {\"course\": \"data-engineering-zoomcamp\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'How to spark standalone cluster is run on windows OS',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Docker engine stopped_failed to fetch extensions',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Run Local Cluster Spark in Windows 10 with CMD',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery2 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery3 = spark\\n.readStream\\n…\\n…\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.',\n",
       "  'section': 'Project',\n",
       "  'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark docker-compose setup',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the “gz” file (no, the “unzip” command won’t work for this.)\\n“gzip -d file.gz”g',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at …\\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n→ Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': '`spark-submit` errors',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Spark Standalone Mode on Windows',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You need to redefine the python environment variable to that of your user account',\n",
       "  'section': 'Project',\n",
       "  'question': 'How to run python as start up script?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(query = query, related_docs = context )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat(prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To run a Spark standalone cluster on Windows OS, follow these steps: \\n\\n1. Change the working directory to the Spark directory:\\n```\\ncd %SPARK_HOME%\\n```\\nIf you haven't set up the `SPARK_HOME` variable, use the path to your Spark installation instead. \\n\\n2. Start the Spark Master by navigating to the bin folder:\\n```\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\n```\\n\\n3. Start the worker nodes by providing the Spark Master URL: \\n```\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\\n```\\n\\nMake sure to replace `localhost:7077` with the appropriate IP address and port if your setup requires a different configuration. \\n\\nThese steps will help you run the Spark engine in standalone mode on a Windows operating system.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
