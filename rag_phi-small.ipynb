{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from minsearch import Index\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/run/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model_id = \"microsoft/Phi-3-small-128k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=\"auto\",\n",
    "  \n",
    "    \n",
    "    trust_remote_code=True\n",
    ")\n",
    "assert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\n",
    "device = torch.cuda.current_device()\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "with open('documents.json', 'r') as file:\n",
    "\n",
    "    docs = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "\n",
    "    for document in doc['documents']:\n",
    "\n",
    "        document['course'] = doc['course'] \n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfields = [\"text\", \"section\", \"question\"]\n",
    "\n",
    "\n",
    "indobject = Index(text_fields = textfields, keyword_fields = ['course'])\n",
    "indobject.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, boost_dict = {\"question\": 3}, filter_dict =  {\"course\":\"mlops-zoomcamp\"}, num_results = 5 ):\n",
    "\n",
    "    context = indobject.search(query = query, boost_dict = boost_dict, filter_dict = filter_dict, num_results = num_results\n",
    "                    )\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, related_docs):\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    You are an assistant for teaching online courses answer the question based on the context provided.\n",
    "    Only the information available in the provided context should be used strictly adhere to this\n",
    "    \n",
    "    question:{query}\n",
    "    \n",
    "    context:{context}\n",
    "    \n",
    "    \n",
    "    \"\"\".strip()\n",
    "    \n",
    "    \n",
    "    context = \"\"\n",
    "    for doc in related_docs:\n",
    "        \n",
    "        context += f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']} \\n\\n\"\n",
    "        \n",
    "    \n",
    "    prompt = prompt.format(query = query, context = context).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, generation_args = {}):\n",
    "    \n",
    "    messages = [{\"role\":\"user\", \"content\": prompt.strip()}]\n",
    "    pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "    if not generation_args:\n",
    "        generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    print(output)\n",
    "    \n",
    "    return output[0]['generated_text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, boost_dict = {\"question\": 3}, course_filter =  {\"course\": \"data-engineering-zoomcamp\"}, generation_args ={}):\n",
    "    \n",
    "    context = search(query = query, boost_dict = boost_dict, filter_dict = course_filter)\n",
    "    \n",
    "    prompt = build_prompt(query, context)\n",
    "    \n",
    "    answer = chat(prompt, generation_args = generation_args)\n",
    "    \n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to run spark engine ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
